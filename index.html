<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Latent Graph Inference with Limited Supervision">
  <meta name="keywords" content="Graph Neural Networks, Graph Structure Learning, Limited Supervision">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Latent Graph Inference with Limited Supervision</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	  showProcessingMessages: false,
	  messageStyle: "none", 
	  extensions: ["tex2jax.js"],
	  jax: ["input/TeX", "output/HTML-CSS"],
	  tex2jax: {
		inlineMath: [ ['$','$'], ["\\(","\\)"] ],
		displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
		skipTags: ['script', 'noscript', 'style', 'textarea', 'pre','code', 'a', 'annotation', 'annotation-xml'],
		ignoreClass: 'crayon-.*'  
	  },
	  'HTML-CSS': {
		  showMathMenu: false
	  }
	});
	MathJax.Hub.Queue(["Typeset",MathJax.Hub]);
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://github.com/Jianglin954/LGI-LS">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Latent Graph Inference with Limited Supervision</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://jianglin954.github.io/">Jianglin Lu</a><sup>1&ast;</sup>&#8192;
              </span>
			  <span class="author-block">
                <a href="https://sites.google.com/view/homepage-of-yi-xu">Yi Xu</a><sup>1</sup>&#8192;
              </span>
              <span class="author-block">
                <a href="https://huanwang.tech/">Huan Wang</a><sup>1</sup>&#8192;
              </span>
              <span class="author-block">
                <a href="https://yueb17.github.io/">Yue Bai</a><sup>1</sup>&#8192;
              </span>
              <span class="author-block">
                <a href="http://www1.ece.neu.edu/~yunfu/">Yun Fu</a><sup>1,2</sup>&#8192;
              </span>
            </div>
            <h1 style="font-size:23px;font-weight:bold">NeurIPS 2023</h1>

            <div class="is-size-5 publication-authors">
              <span><sup>1</sup>Department of Electrical and Computer Engineering, Northeastern University&#8192;</span><br>
              <span><sup>2</sup>Khoury College of Computer Science, Northeastern University</span><br>
            </div>

            <div style="font-size:15px">
              <span><sup>&ast;</sup>Corresponding author: <a href="mailto:jianglinlu@outlook.com">jianglinlu@outlook.com</a></span></br>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://openreview.net/forum?id=tGuMwFnRZX"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Openreview</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2310.04314.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>ArXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/Jianglin954/LGI-LS"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div align='center'>
		
			<p align="center">
				<table><tr>
				<td><a><img src="./static/images/fig5.jpg"  width="400" ></a></td>
				<td>\begin{equation} \qquad\quad \end{equation}</td>
				<td>
					\begin{equation}   
			\mathbf{A}: \begin{array}{lll}
			& \begin{array}{llllll}\ \  1 & 2 & 3 & 4 & 5 & 6 \end{array} & \\
			\begin{array}{l}  1 \\ 2 \\ 3 \\ 4 \\ 5 \\ 6 \end{array}&
			\left[\begin{array}{llllll}
				1 & 0 & 1 & 0 & 1 & 0 \\  
				0 & 1 & 1 & 1 & 0 & 0 \\ 
				1 & 1 & 1 & 0 & 0 & 1 \\ 
				0 & 1 & 0 & 1 & 0 & 0 \\ 
				1 & 0 & 0 & 0 & 1 & 0 \\ 
				0 & 0 & 1 & 0 & 0 & 1 \\ 
			\end{array}\right]
			\begin{array}{l}  
			\end{array}
		\end{array} \nonumber
		\end{equation}
				</td>
				</tr><table>
			</p>

			\begin{equation}   
			\mathbf{C} : \begin{array}{lll}
			& \begin{array}{ll}\ \ \  2 & 4 \end{array} & \\
			\begin{array}{l}  1 \\ 2 \\ 3 \\ 4 \\ 5 \\ 6 \end{array}&
			\left[\begin{array}{llll}
			0  & 0  \\  
			1  & 1  \\ 
			1  & 0  \\ 
			1  & 1  \\ 
			0  & 0  \\ 
			0  & 0  \\ 
			\end{array}\right]
			\end{array}; \quad\qquad
			\mathbf{R} : \begin{array}{lll}
			& \begin{array}{llllll} \ \ 1 & 2 &  3 &  4 &  5 &  6 \end{array} & \\
			\begin{array}{l}  1 \\ 5 \\ 6 \end{array}&
			\left[\begin{array}{llllll}
			1 & 0 & 1 & 0 & 1 & 0 \\  
			1 & 0 & 0 & 0 & 1 & 0 \\ 
			0 & 0 & 1 & 0 & 0 & 1 \\ 
			\end{array}\right]
			\end{array}; \quad\qquad
			\mathbf{U} : \begin{array}{lll}
			& \begin{array}{ll} \ \ 2 & 4 \end{array} & \\
			\begin{array}{l}  1 \\ 5 \\ 6 \end{array}&
			\left[\begin{array}{ll}
			0 & 0 \\  
			0 & 0 \\ 
			0 & 0 \\ 
			\end{array}\right]
			\end{array}
			\nonumber
			\end{equation}
        </div>
        <div class="content has-text-justified">
		Illustration of k-hop starved nodes on the Cora and Citeseer datasets (the suffix number in figure represents the number of labeled nodes). 
		We calculate the number of k-hop starved nodes for k=1, 2, 3, 4 based on their original graph topologies. 
		Obviously, the number of k-hop starved nodes decreases as the value of k increases. 
		This can be explained by the fact that as k increases, the nodes have more neighbors (from 1- to k-hop), and the possibility of having at least one labeled neighbor increases.
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
			Latent graph inference (LGI) aims to jointly learn the underlying graph structure and node representations from data features. 
			However, existing LGI methods commonly suffer from the issue of supervision starvation, where massive edge weights are learned without semantic supervision and do not contribute to the training loss.
			Consequently, these supervision-starved weights, which may determine the predictions of testing samples, cannot be semantically optimal, resulting in poor generalization.  
			In this paper, we observe that this issue is actually caused by the graph sparsification operation, which severely destroys the important connections established between pivotal nodes and labeled ones.
			To address this, we propose to restore the corrupted affinities and replenish the missed supervision for better LGI.
			The key challenge then lies in identifying the critical nodes and recovering the corrupted affinities.
			We begin by defining the pivotal nodes as k-hop starved nodes, which can be identified based on a given adjacency matrix.
			Considering the high computational burden, we further present a more efficient alternative inspired by CUR matrix decomposition. 
			Subsequently, we eliminate the starved nodes by reconstructing the destroyed connections.
			Extensive experiments on representative benchmarks demonstrate that reducing the starved nodes consistently improves the performance of state-of-the-art LGI methods, especially under extremely limited supervision (6.12% improvement on Pubmed with a labeling rate of only 0.3%). 
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
			<h2 class="title is-3" align='center'>Latent Graph Inference</h2>
				<div class="content has-text-justified">
					Given a graph $\mathcal{G}(\mathcal{V}, \mathbf{X} )$ containing $n$ nodes $\mathcal{V}=\{V_1, \ldots, V_n\}$ and a feature matrix $\mathbf{X} \in \mathbb{R}^{n\times d}$ with each row $\mathbf{X}_{i:} \in \mathbb{R}^d$ representing the $d$-dimensional attributes of node $V_i$, latent graph inference (LGI) aims to simultaneously learn the underlying graph topology encoded by an adjacency matrix $\mathbf{A} \in \mathbb{R}^{n\times n}$ and the discriminative $d'$-dimensional node representations $\mathbf{Z} \in \mathbb{R}^{n\times d'}$ based on $\mathbf{X}$, where the learned $\mathbf{A}$ and $\mathbf{Z}$ are jointly optimal for certain downstream tasks $\mathcal{T}$ given a specific loss function $\mathcal{L}$. 
				</div>
				</br>
				</br>
				</br>

			<h2 class="title is-3" align='center'>Supervision Starvation</h2>
				<div class="content has-text-justified">
				To illustrate the supervision starvation problem~\cite{SLAPS}, we consider a general LGI model $\mathcal{M}$ consisting of a latent graph generator $\mathcal{P}_{\mathbf{\Phi}}$ and a node encoder $\mathcal{F}_{\mathbf{\Theta}}$. 
				For simplicity, we ignore the activation function and assume that $\mathcal{F}_{\mathbf{\Theta}}$ is implemented using a $1$-layer GNN, \textit{i.e.}, $\mathcal{F}_{\mathbf{\Theta}}=\mathtt{GNN}_1(\mathbf{X}, \mathbf{A}; \mathbf{\Theta})$, where $\mathbf{A}=\mathcal{P}_{\mathbf{\Phi}}(\mathbf{X})$. 
				For each node $\mathbf{X}_{i:}$, the corresponding node representation $\mathbf{Z}_{i:}$ learned by the model $\mathcal{M}$ can be expressed as:
				\begin{equation}
				\mathbf{Z}_{i:} = \mathbf{A}_{i:}\mathbf{X}\mathbf{\Theta} = \left(\sum_{j \in \Omega} \mathbf{A}_{ij}\mathbf{X}_{j:} \right)\mathbf{\Theta},
				\end{equation}
				where $\Omega=\{j\ |\  \mathbb{1}_{\mathbb{R}^+}(\mathbf{A})_{ij}=1 \}$ and $\mathbf{A}_{ij}=\mathcal{P}_{\mathbf{\Phi}}(\mathbf{X}_{i:}, \mathbf{X}_{j:})$. Consider the node classification loss:
				\begin{equation}
				\min_{\mathbf{A}, \mathbf{\Theta}} \mathcal{L} = \sum_{i\in \mathcal{Y}_{L}} \sum_{j=1}^{|\mathcal{C}|} \mathbf{Y}_{ij} \ln \mathbf{Z}_{ij} = \sum_{i\in \mathcal{Y}_{L}} \mathbf{Y}_{i:} \ln \mathbf{Z}_{i:}^{\top} = \sum_{i\in \mathcal{Y}_{L}} \mathbf{Y}_{i:} \ln \left( \left(\sum_{j \in \Omega} \mathbf{A}_{ij}\mathbf{X}_{j:} \right)\mathbf{\Theta}\right)^\top,
				\end{equation} 
				where $\mathcal{Y}_{L}$ represents the set of indexes of labeled nodes and $|\mathcal{C}|$ denotes the size of label set. 
				For $\forall i\in \mathcal{Y}_{L}$, $j \in \Omega$, $\mathbf{A}_{ij}$ is optimized via backpropagation under the supervision of label $\mathbf{Y}_{i:}$. 
				For $\forall i \notin \mathcal{Y}_{L}$, however, if $j \notin \mathcal{Y}_{L}$ for $\forall j \in \Omega$, $\mathbf{A}_{ij}$ will receive no supervision from any label and, as a result, cannot be semantically optimal after training. 
				Consequently, the learning models exhibit poor generalization as the predictions of testing nodes inevitably rely on these supervision-starved weights. 
				This phenomenon is referred to as \textit{supervision starvation} (SS), where many edge weights are learned without any label supervision.
				</div>

				<a><img src="./static/images/fig1.jpg"  width="500" ></a>
				<div class="content has-text-justified">
				Illustration of k-hop starved nodes on the Cora and Citeseer datasets (the suffix number in figure represents the number of labeled nodes). 
				We calculate the number of k-hop starved nodes for k=1, 2, 3, 4 based on their original graph topologies. 
				Obviously, the number of k-hop starved nodes decreases as the value of k increases. 
				This can be explained by the fact that as k increases, the nodes have more neighbors (from 1- to k-hop), and the possibility of having at least one labeled neighbor increases.
				</div>

				</br>
				</br>
				</br>



			<h2 class="title is-3" align='center'>How to Identify Starved Nodes</h2>    
				Compression 

				</br>
				</br>
				</br>


			
			<h2 class="title is-3" align='center'>How to Eliminate Starved Nodes</h2>
				
				Compression 

				</br>
				</br>
				</br>


			<h2 class="title is-3" align='center'>Experiments</h2>
				<p align="center">
					<table><tr>
					<td><a><img src="./static/images/table1.jpg"  height="100" ></a></td>
					</tr><table>
				</p>
				</br>
				</br>
				<p align="center">
					<table><tr>
					<td><a><img src="./static/images/fig3.jpg"  height="100" ></a></td>
					<td><a><img src="./static/images/fig4.jpg"  height="100" ></a></td>
					<td><a><img src="./static/images/fig2.jpg"  height="100" ></a></td>
					</tr><table>
				</p>
     
        </div>
      </div>
    </div>
  </section>





<section class="section" id="BibTeX">
<div align="left">
	<h2 class="title">BibTeX</h2>
	<pre><code>@inproceedigs{Jianglin2023LGI,
title={Latent Graph Inference with Limited Supervision},
author={Jianglin Lu, Yi Xu, Huan Wang, Yue Bai, Yun Fu},
booktitle={Advances in Neural Information Processing Systems},
year={2023}
}
</code></pre>
    </div>
  </section>


    <div align="center">
        <div class="content">
          This website is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
        </div>
    </div>

</body>

</html>
