<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Latent Graph Inference with Limited Supervision">
  <meta name="keywords" content="Graph Neural Networks, Graph Structure Learning, Limited Supervision">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Latent Graph Inference with Limited Supervision</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
</head>

<body width: 100%;>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://github.com/Jianglin954/LGI-LS">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Latent Graph Inference with Limited Supervision</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://jianglin954.github.io/">Jianglin Lu</a><sup>1&ast;</sup>&#8192;
              </span>
			  <span class="author-block">
                <a href="https://sites.google.com/view/homepage-of-yi-xu">Yi Xu</a><sup>1</sup>&#8192;
              </span>
              <span class="author-block">
                <a href="https://huanwang.tech/">Huan Wang</a><sup>1</sup>&#8192;
              </span>
              <span class="author-block">
                <a href="https://yueb17.github.io/">Yue Bai</a><sup>1</sup>&#8192;
              </span>
              <span class="author-block">
                <a href="http://www1.ece.neu.edu/~yunfu/">Yun Fu</a><sup>1,2</sup>&#8192;
              </span>
            </div>
            <h1 style="font-size:23px;font-weight:bold">NeurIPS 2023</h1>

            <div class="is-size-5 publication-authors">
              <span><sup>1</sup>Department of Electrical and Computer Engineering, Northeastern University&#8192;</span><br>
              <span><sup>2</sup>Khoury College of Computer Science, Northeastern University</span><br>
            </div>

            <div style="font-size:15px">
              <span><sup>&ast;</sup>Corresponding author: <a href="mailto:jianglinlu@outlook.com">jianglinlu@outlook.com</a></span></br>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://openreview.net/forum?id=tGuMwFnRZX"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Openreview</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2310.04314.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>ArXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/Jianglin954/LGI-LS"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div align='center'>
          <a><img src="./static/images/fig1.jpg"  height="75" ></a>
        </div>
        <div class="content has-text-justified">
		Illustration of k-hop starved nodes on different datasets. Obviously, the number of k-hop starved nodes decreases as the value of k increases. To provide an intuitive perspective, we use two real-world graph datasets, namely Cora ($2708$ nodes) and Citeseer ($3327$ nodes), as examples. We calculate the number of $k$-hop starved nodes for $k=1, 2, 3, 4$, based on their original graph topology. 
Fig. \ref{fig1} shows the statistical results for the Cora140, Cora390, Citesser120, and Citeseer370 datasets, where the suffix number represents the number of labeled nodes. 
From Fig. \ref{fig1}, we observe that as the value of $k$ increases, the number of starved nodes decreases. 
This can be explained by the fact that as $k$ increases, the nodes have more neighbors (from $1$- to $k$-hop), and the possibility of having at least one labeled neighbor increases. 
Adopting a deeper GNN (larger $k$) can thus mitigate the SS problem. 
However, it is important to consider that deeper GNNs result in higher computational consumption and may lead to poorer generalization performance~\cite{Li2018, Kenta2020, Alon2021}. 
Furthermore, as shown in Fig. \ref{fig1}, even with a $4$-layer GNN, there are still hundreds of $4$-hop starved nodes in the Citesser120. 
Therefore, we believe that employing a deeper GNN is not the optimal solution to resolve the SS problem.

        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
			Latent graph inference (LGI) aims to jointly learn the underlying graph structure and node representations from data features. 
			However, existing LGI methods commonly suffer from the issue of <i>supervision starvation</i>, where massive edge weights are learned without semantic supervision and do not contribute to the training loss.
			Consequently, these supervision-starved weights, which may determine the predictions of testing samples, cannot be semantically optimal, resulting in poor generalization.  
			In this paper, we observe that this issue is actually caused by the graph sparsification operation, which severely destroys the important connections established between pivotal nodes and labeled ones.
			To address this, we propose to <i>restore the corrupted affinities and replenish the missed supervision for better LGI</i>.
			The key challenge then lies in identifying the critical nodes and recovering the corrupted affinities.
			We begin by defining the pivotal nodes as <i>k-hop starved nodes</i>, which can be identified based on a given adjacency matrix.
			Considering the high computational burden, we further present a more efficient alternative inspired by <i>CUR matrix decomposition</i>. 
			Subsequently, we eliminate the starved nodes by reconstructing the destroyed connections.
			Extensive experiments on representative benchmarks demonstrate that reducing the starved nodes consistently improves the performance of state-of-the-art LGI methods, especially under extremely limited supervision (6.12% improvement on Pubmed with a labeling rate of only 0.3%). 
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">

		  <h2 class="title is-3" align='center'>Latent Graph Inference</h2>
		  Given a graph $\mathcal{G}(\mathcal{V}, \mathbf{X} )$ containing $n$ nodes $\mathcal{V}=\{V_1, \ldots, V_n\}$ and a feature matrix $\mathbf{X} \in \mathbb{R}^{n\times d}$ with each row $\mathbf{X}_{i:} \in \mathbb{R}^d$ representing the $d$-dimensional attributes of node $V_i$, latent graph inference (LGI) aims to simultaneously learn the underlying graph topology encoded by an adjacency matrix $\mathbf{A} \in \mathbb{R}^{n\times n}$ and the discriminative $d'$-dimensional node representations $\mathbf{Z} \in \mathbb{R}^{n\times d'}$ based on $\mathbf{X}$, where the learned $\mathbf{A}$ and $\mathbf{Z}$ are jointly optimal for certain downstream tasks $\mathcal{T}$ given a specific loss function $\mathcal{L}$. 

		</br>
		</br>
		</br>

		  <h2 class="title is-3" align='center'>Supervision Starvation</h2>
		
		To illustrate the supervision starvation problem, we consider a general LGI model `\mathcal{M}` consisting of a latent graph generator `\mathcal{P}_{\mathbf{\Phi}}` and a node encoder $\mathcal{F}_{\mathbf{\Theta}}$. 
		For simplicity, we ignore the activation function and assume that $\mathcal{F}_{\mathbf{\Theta}}$ is implemented using a $1$-layer GNN, \textit{i.e.}, $\mathcal{F}_{\mathbf{\Theta}}=\mathtt{GNN}_1(\mathbf{X}, \mathbf{A}; \mathbf{\Theta})$, where $\mathbf{A}=\mathcal{P}_{\mathbf{\Phi}}(\mathbf{X})$. 
		For each node $\mathbf{X}_{i:}$, the corresponding node representation $\mathbf{Z}_{i:}$ learned by the model $\mathcal{M}$ can be expressed as:
		\begin{equation}
		\mathbf{Z}_{i:} = \mathbf{A}_{i:}\mathbf{X}\mathbf{\Theta} = \left(\sum_{j \in \Omega} \mathbf{A}_{ij}\mathbf{X}_{j:} \right)\mathbf{\Theta},
		\end{equation}
		where $\Omega=\{j\ |\  \mathbbm{1}_{\mathbb{R}^+}(\mathbf{A})_{ij}=1 \}$ and $\mathbf{A}_{ij}=\mathcal{P}_{\mathbf{\Phi}}(\mathbf{X}_{i:}, \mathbf{X}_{j:})$. Consider the node classification loss:
		\begin{equation}
		\min_{\mathbf{A}, \mathbf{\Theta}} \mathcal{L} = \sum_{i\in \mathcal{Y}_{L}} \sum_{j=1}^{|\mathcal{C}|} \mathbf{Y}_{ij} \ln \mathbf{Z}_{ij} = \sum_{i\in \mathcal{Y}_{L}} \mathbf{Y}_{i:} \ln \mathbf{Z}_{i:}^{\top} = \sum_{i\in \mathcal{Y}_{L}} \mathbf{Y}_{i:} \ln \left( \left(\sum_{j \in \Omega} \mathbf{A}_{ij}\mathbf{X}_{j:} \right)\mathbf{\Theta}\right)^\top,
		\end{equation} 
		where $\mathcal{Y}_{L}$ represents the set of indexes of labeled nodes and $|\mathcal{C}|$ denotes the size of label set. 
		For $\forall i\in \mathcal{Y}_{L}$, $j \in \Omega$, $\mathbf{A}_{ij}$ is optimized via backpropagation under the supervision of label $\mathbf{Y}_{i:}$. 
		For $\forall i \notin \mathcal{Y}_{L}$, however, if $j \notin \mathcal{Y}_{L}$ for $\forall j \in \Omega$, $\mathbf{A}_{ij}$ will receive no supervision from any label and, as a result, cannot be semantically optimal after training. 
		Consequently, the learning models exhibit poor generalization as the predictions of testing nodes inevitably rely on these supervision-starved weights. 
		This phenomenon is referred to as \textit{supervision starvation} (SS), where many edge weights are learned without any label supervision. 
		It is easy to infer that this issue also persists in a $k$-layer GNN. 

          <p align="center">
            <table><tr>
            <td><a><img src="./static/images/table1.jpg"  height="100" ></a></td>
            </tr><table>
          </p>
		  <p align="center">
            <table><tr>
            <td><a><img src="./static/images/GCNKNN.pdf"  height="100" ></a></td>
			<td><a><img src="./static/images/GRCN.pdf"  height="100" ></a></td>
			<td><a><img src="./static/images/SLAPS.pdf"  height="100" ></a></td>
            </tr><table>
          </p>
    Performances 
          </br>
          </br>
          </br>

          <h2 class="title is-3" align='center'>A New Network Compression Paradigm</h2>
           

          <p align="center">
            <table><tr>
            <td><a><img src="figs/plot_cifar10_resnet.svg"  height="100" ></a></td>
            <td><a><img src="figs/plot_cifar100_resnet.svg"  height="100" ></a></td>
            </tr><table>
          </p>
    Compression 

          

        
        </div>
      </div>
    </div>
  </section>





  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedigs{Jianglin2023LGI,
	title={Latent Graph Inference with Limited Supervision},
	author={Jianglin Lu, Yi Xu, Huan Wang, Yue Bai, Yun Fu},
	booktitle={Advances in Neural Information Processing Systems},
	year={2023}
  }
</code></pre>
    </div>
  </section>



  <footer class="footer">
    <div align="center" class="container">
      <div class="columns is-centered">
        <div class="content">
          This website is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
