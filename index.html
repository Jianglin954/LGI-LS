<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Latent Graph Inference with Limited Supervision">
  <meta name="keywords" content="Graph Neural Networks, Graph Structure Learning, Limited Supervision">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Latent Graph Inference with Limited Supervision</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" async></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://github.com/yueb17/PEMN">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Latent Graph Inference with Limited Supervision</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://jianglin954.github.io/">Jianglin Lu</a><sup>1&ast;</sup>&#8192;
              </span>
			  <span class="author-block">
                <a href="https://sites.google.com/view/homepage-of-yi-xu">Yi Xu</a><sup>1</sup>&#8192;
              </span>
              <span class="author-block">
                <a href="https://huanwang.tech/">Huan Wang</a><sup>1</sup>&#8192;
              </span>
              <span class="author-block">
                <a href="https://yueb17.github.io/">Yue Bai</a><sup>1</sup>&#8192;
              </span>
              <span class="author-block">
                <a href="http://www1.ece.neu.edu/~yunfu/">Yun Fu</a><sup>1,2</sup>&#8192;
              </span>
            </div>
            <h1 style="font-size:23px;font-weight:bold">NeurIPS 2023</h1>

            <div class="is-size-5 publication-authors">
              <span><sup>1</sup>Department of Electrical and Computer Engineering, Northeastern University&#8192;</span><br>
              <span><sup>2</sup>Khoury College of Computer Science, Northeastern University</span><br>
            </div>

            <div style="font-size:15px">
              <span><sup>&ast;</sup>Corresponding author: <a href="mailto:jianglinlu@outlook.com">jianglinlu@outlook.com</a></span></br>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://openreview.net/forum?id=tGuMwFnRZX"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Openreview</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2310.04314.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>ArXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/Jianglin954/LGI-LS"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div align='center'>
          <a><img src="figs/main_figure_cr.svg"  height="200" ></a>
        </div>
        <div class="content has-text-justified">
          Left is \begin{equation} a = b + c \end{equation}
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
			Latent graph inference (LGI) aims to jointly learn the underlying graph structure and node representations from data features. 
			However, existing LGI methods commonly suffer from the issue of <i>supervision starvation</i>, where massive edge weights are learned without semantic supervision and do not contribute to the training loss.
			Consequently, these supervision-starved weights, which may determine the predictions of testing samples, cannot be semantically optimal, resulting in poor generalization.  
			In this paper, we observe that this issue is actually caused by the graph sparsification operation, which severely destroys the important connections established between pivotal nodes and labeled ones.
			To address this, we propose to <i>restore the corrupted affinities and replenish the missed supervision for better LGI</i>.
			The key challenge then lies in identifying the critical nodes and recovering the corrupted affinities.
			We begin by defining the pivotal nodes as <i>k-hop starved nodes</i>, which can be identified based on a given adjacency matrix.
			Considering the high computational burden, we further present a more efficient alternative inspired by <i>CUR matrix decomposition</i>. 
			Subsequently, we eliminate the starved nodes by reconstructing the destroyed connections.
			Extensive experiments on representative benchmarks demonstrate that reducing the starved nodes consistently improves the performance of state-of-the-art LGI methods, especially under extremely limited supervision (6.12% improvement on Pubmed with a labeling rate of only 0.3%). 
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">

          
          <h2 class="title is-3" align='center'>Background</h2>
		  <h3 class="title is-3" align='center'>Latent Graph Inference</h3>
		  Given a graph $\mathcal{G}(\mathcal{V}, \mathbf{X} )$ containing $n$ nodes $\mathcal{V}=\{V_1, \ldots, V_n\}$ and a feature matrix $\mathbf{X} \in \mathbb{R}^{n\times d}$ with each row $\mathbf{X}_{i:} \in \mathbb{R}^d$ representing the $d$-dimensional attributes of node $V_i$, latent graph inference (LGI) aims to simultaneously learn the underlying graph topology encoded by an adjacency matrix $\mathbf{A} \in \mathbb{R}^{n\times n}$ and the discriminative $d'$-dimensional node representations $\mathbf{Z} \in \mathbb{R}^{n\times d'}$ based on $\mathbf{X}$, where the learned $\mathbf{A}$ and $\mathbf{Z}$ are jointly optimal for certain downstream tasks $\mathcal{T}$ given a specific loss function $\mathcal{L}$. 

		  <h3 class="title is-3" align='center'>Supervision Starvation</h3>
		
		To illustrate the supervision starvation problem~\cite{SLAPS}, we consider a general LGI model $\mathcal{M}$ consisting of a latent graph generator $\mathcal{P}_{\mathbf{\Phi}}$ and a node encoder $\mathcal{F}_{\mathbf{\Theta}}$. 
		For simplicity, we ignore the activation function and assume that $\mathcal{F}_{\mathbf{\Theta}}$ is implemented using a $1$-layer GNN, \textit{i.e.}, $\mathcal{F}_{\mathbf{\Theta}}=\mathtt{GNN}_1(\mathbf{X}, \mathbf{A}; \mathbf{\Theta})$, where $\mathbf{A}=\mathcal{P}_{\mathbf{\Phi}}(\mathbf{X})$. 
		For each node $\mathbf{X}_{i:}$, the corresponding node representation $\mathbf{Z}_{i:}$ learned by the model $\mathcal{M}$ can be expressed as:
		\begin{equation}
		\mathbf{Z}_{i:} = \mathbf{A}_{i:}\mathbf{X}\mathbf{\Theta} = \left(\sum_{j \in \Omega} \mathbf{A}_{ij}\mathbf{X}_{j:} \right)\mathbf{\Theta},
		\end{equation}
		where $\Omega=\{j\ |\  \mathbbm{1}_{\mathbb{R}^+}(\mathbf{A})_{ij}=1 \}$ and $\mathbf{A}_{ij}=\mathcal{P}_{\mathbf{\Phi}}(\mathbf{X}_{i:}, \mathbf{X}_{j:})$. Consider the node classification loss:
		\begin{equation}
		\min_{\mathbf{A}, \mathbf{\Theta}} \mathcal{L} = \sum_{i\in \mathcal{Y}_{L}} \sum_{j=1}^{|\mathcal{C}|} \mathbf{Y}_{ij} \ln \mathbf{Z}_{ij} = \sum_{i\in \mathcal{Y}_{L}} \mathbf{Y}_{i:} \ln \mathbf{Z}_{i:}^{\top} = \sum_{i\in \mathcal{Y}_{L}} \mathbf{Y}_{i:} \ln \left( \left(\sum_{j \in \Omega} \mathbf{A}_{ij}\mathbf{X}_{j:} \right)\mathbf{\Theta}\right)^\top,
		\end{equation} 
		where $\mathcal{Y}_{L}$ represents the set of indexes of labeled nodes and $|\mathcal{C}|$ denotes the size of label set. 
		For $\forall i\in \mathcal{Y}_{L}$, $j \in \Omega$, $\mathbf{A}_{ij}$ is optimized via backpropagation under the supervision of label $\mathbf{Y}_{i:}$. 
		For $\forall i \notin \mathcal{Y}_{L}$, however, if $j \notin \mathcal{Y}_{L}$ for $\forall j \in \Omega$, $\mathbf{A}_{ij}$ will receive no supervision from any label and, as a result, cannot be semantically optimal after training. 
		Consequently, the learning models exhibit poor generalization as the predictions of testing nodes inevitably rely on these supervision-starved weights. 
		This phenomenon is referred to as \textit{supervision starvation} (SS), where many edge weights are learned without any label supervision. 
		It is easy to infer that this issue also persists in a $k$-layer GNN. 

          <p align="center">
            <table><tr>
            <td><a><img src="./static/images/fig1.jpg"  height="100" ></a></td>
            </tr><table>
          </p>
    Performances of ConvMixer (top row) and ViT (bottom row) backbones on CIFAR10 dataset with different model hyperparameters. Y-axis represent the test accuracy and X-axis denotes different network parameter settings. Dense means the model is trained in regular fashion. Mask is the sparse selection strategy.
One-layer, MP, and RP are our strategies. The decimal after RP means the number of unique
parameters compared with MP. From Mask to RP 1e-5, the unique values of network decrease.
Different experimental settings illustrate the representative potential of random weights.
          </br>
          </br>
          </br>

          <h2 class="title is-3" align='center'>A New Network Compression Paradigm</h2>
           

          <p align="center">
            <table><tr>
            <td><a><img src="figs/plot_cifar10_resnet.svg"  height="100" ></a></td>
            <td><a><img src="figs/plot_cifar100_resnet.svg"  height="100" ></a></td>
            </tr><table>
          </p>
    Compression performance validation on CIFAR10 (left) and CIFAR100 (right) datasets on
ResNet32/ResNet56 backbones. Y-axis denotes the test accuracy. X-axis means the network size
compression ratio. Different colors represent different network architectures. The straight lines on the
top are performance of dense model with regular training. Lines with different symbol shapes denote
different settings. For ResNet, our three points are based on MP, RP 1e-1, and RP 1e-2, respectively.
This pair of figures show that our proposed paradigm achieves admirable compression performance
compared with baselines. In very high compression ratios, we can still maintain the test accuracy.

          

        
        </div>
      </div>
    </div>
  </section>





  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedigs{Jianglin2023LGI,
	title={Latent Graph Inference with Limited Supervision},
	author={Jianglin Lu, Yi Xu, Huan Wang, Yue Bai, Yun Fu},
	booktitle={Advances in Neural Information Processing Systems},
	year={2023}
  }
</code></pre>
    </div>
  </section>



  <footer class="footer">
    <div align="center" class="container">
      <div class="columns is-centered">
        <div class="content">
          This website is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
